{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I am going to use spacy for entity recognition on 200 Resume \n",
    "and experiment around various NLP tools for text analysis. \n",
    "\n",
    "I have also added skills match feature so that hiring managers can follow a metric that will\n",
    "help them to decide whether they should move to the interview stage or not. \n",
    "\n",
    "I will be using two datasets; the first contains resume texts \n",
    "and the second contains skills that we will use to create an entity ruler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inside the CSV\n",
    "* ID: Unique identifier and file name for the respective pdf.\n",
    "* Resume_str : Contains the resume text only in string format.\n",
    "* Resume_html : Contains the resume data in html format as present while web scrapping.\n",
    "* Category : Category of the job the resume was used to apply.\n",
    "\n",
    "## Present categories\n",
    "HR, Designer, Information-Technology, Teacher, Advocate, Business-Development, Healthcare, Fitness, Agriculture, BPO, Sales, Consultant, Digital-Media, Automobile, Chef, Finance, Apparel, Engineering, Accountant, Construction, Public-Relations, Banking, Arts, Aviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jobzilla skill patterns\n",
    "\n",
    "The jobzilla skill dataset is jsonl file containing different skills that can be used to create spaCy entity_ruler. \n",
    "\n",
    "The data set contains label and pattern-> diferent words used to descibe skills in various resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "#gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "#Visualization\n",
    "from spacy import displacy\n",
    "import pyLDAvis.gensim_models\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Data loading/ Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "\n",
    "#nltk\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['stopwords','wordnet'])\n",
    "\n",
    "#warning\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "In this section, I am going to load the spaCy model, Resume Dataset, and Jobzilla skills dataset directly into the entity ruler.\n",
    "\n",
    "### Resume Dataset\n",
    "Using Pandas read_csv to read dataset containing text data about Resume.\n",
    "\n",
    "* I am going to randomized Job categories so that 200 samples contain various job categories instead of one.\n",
    "* I am going to limit our number of samples to 200 as processing 2400+ takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Getting The Data/Resume.csv\")\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "data = df.copy().iloc[0:200,]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading spaCy model\n",
    "I can download spaCy model then load spacy model into nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "skill_pattern_path = \"jz_skill_patterns.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Ruler\n",
    "To create an entity ruler we need to add a pipeline and then load the .jsonl file containing skills into ruler.\n",
    "\n",
    "As you can see we have successfully added a new pipeline entity_ruler. \n",
    "\n",
    "Entity ruler helps us add additional rules to highlight various categories within the text, such as skills and job description in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills\n",
    "\n",
    "I will create two python functions to extract all the skills within a resume and create an array containing all the skills. \n",
    "\n",
    "Later I am going to apply this function to the dataset and create a new feature called skill. \n",
    "\n",
    "This will help us visualize trends and patterns within the dataset.\n",
    "\n",
    "get_skills is going to extract skills from a single text.\n",
    "unique_skills will remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Resume Text\n",
    "\n",
    "I am going to use nltk library to clean our dataset in a few steps:\n",
    "\n",
    "* I am going to use regex to remove hyperlinks, special characters, or punctuations.\n",
    "* Lowering text\n",
    "* Splitting text into array based on space\n",
    "* Lemmatizing text to its base form for normalizations\n",
    "* Removing English stopwords\n",
    "* Appending the results into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = []\n",
    "for i in range(data.shape[0]):\n",
    "    review = re.sub('(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"',\" \", data[\"Resume_str\"].iloc[i],)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    lm = WordNetLemmatizer()\n",
    "    review = [\n",
    "        lm.lemmatize(word)\n",
    "        for word in review\n",
    "        if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    review = \" \".join(review)\n",
    "    clean.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying functions\n",
    "\n",
    "In this section, we are going to apply all the functions we have created previously\n",
    "\n",
    "* creating Clean_Resume columns and adding cleaning Resume data.\n",
    "* creating skills columns, lowering text, and applying the get_skills function.\n",
    "* removing duplicates from skills columns.\n",
    "\n",
    "As you can see below that I have cleaned the resume and skills columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Clean_Resume\"] = clean\n",
    "data[\"skills\"] = data[\"Clean_Resume\"].str.lower().apply(get_skills)\n",
    "data[\"skills\"] = data[\"skills\"].apply(unique_skills)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Now that we have everything we want, we are going to visualize Job distributions and skill distributions.\n",
    "\n",
    "## Jobs Distribution\n",
    "As we can see our random 200 samples contain a variety of job categories. Accountants, Business development, and Advocates are the top categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data, x=\"Category\", title=\"Distribution of Jobs Categories\"\n",
    "    ).update_xaxes(categoryorder=\"total descending\")\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4c10e2b9e2b6f589c2872ec43ed0f8a2d726bd41c661fccef1773aaf1be1447"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
